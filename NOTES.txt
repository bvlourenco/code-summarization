    '''
    TODO: For model training, check:
    https://nlp.seas.harvard.edu/annotated-transformer/
    https://medium.com/@hunter-j-phillips/putting-it-all-together-the-implemented-transformer-bfb11ac1ddfe

    -- (LATER) For evaluation, check:
        https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation
        https://arxiv.org/pdf/2107.07112.pdf
    
    AFTER BASELINE IS DONE:
    - Experimentar técnicas diferentes de tokenization (Word, Subword, and Character-Based Tokenization)
    --  Check tokenization techniques 
        https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17
        Good paper: https://openreview.net/pdf?id=htL4UZ344nF
                    https://openreview.net/forum?id=htL4UZ344nF
    - Aplicar beam search em vez de greedy decoding no processo de inference
    --  In inference time, use beam search instead of greedy decoding. Explanation:
        https://datascience.stackexchange.com/a/93146
    - Aplicar copy attention e relative positional encoding
    - Aplicar as alterações que proponho no paper (2 encoders; token, statement, data-flow and control-flow adjacency matrices; Pre-trained model)
    - Check Kaiming initialization
        https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79
    

    - (LATER) Deploying Pytorch in python via a REST API with Flask: https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html?highlight=transformer
    - (LATER) DO not load all dataset into a list (optional improvement)

    General tutorial with Transformer model, training, evaluation and inference:
    https://pytorch.org/tutorials/beginner/translation_transformer.html?highlight=transformer
    '''
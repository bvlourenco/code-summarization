    '''
    TODO: For model training, check:
    https://nlp.seas.harvard.edu/annotated-transformer/
    https://medium.com/@hunter-j-phillips/putting-it-all-together-the-implemented-transformer-bfb11ac1ddfe

    -- (LATER) For evaluation, check:
        https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation
        https://arxiv.org/pdf/2107.07112.pdf
    
    AFTER BASELINE IS DONE:
    - Check Kaiming initialization (KINDA DONE)
        https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79
    - Pytorch link: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_uniform_


    - Experimentar t√©cnicas diferentes de tokenization (Word, Subword, and Character-Based Tokenization)
    --  Check tokenization techniques 
        https://towardsdatascience.com/word-subword-and-character-based-tokenization-know-the-difference-ea0976b64e17
        Good paper: https://openreview.net/pdf?id=htL4UZ344nF
                    https://openreview.net/forum?id=htL4UZ344nF
    
    -- Check: https://github.com/cedricrupb/code_tokenize (to tokenize source code)
              Hugging Face tokenizer class (https://web.archive.org/web/20211026153231/https://huggingface.co/transformers/tokenizer_summary.html)
                                           (https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaTokenizer)
                                           (https://github.com/microsoft/CodeBERT/blob/master/CodeBERT/code2nl/run.py)
              Byte Pair Encoding explanation (https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0)
              Implementation of Byte Pair Encoding (https://github.com/DolbyUUU/byte_pair_encoding_BPE_subword_tokenization_implementation_python/blob/main/BPE.py)
    -- Also check pre-processing techniques I suggested in the Thesis Project

    - Aplicar copy attention (est√° com BUGS)
    
    - Testar o modelo com os datasets apresentados aqui: https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation

    - (LATER) Deploying Pytorch in python via a REST API with Flask: https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html?highlight=transformer
    - (LATER) DO not load all dataset into a list (optional improvement)

    General tutorial with Transformer model, training, evaluation and inference:
    https://pytorch.org/tutorials/beginner/translation_transformer.html?highlight=transformer

    TODO: Ver torch.compile!!!

    Depois:

    - VER LATER: Na tokeniza√ß√£o, devo retirar sinais de pontua√ß√£o ou n√£o? (Tenho estes sinais de pontua√ß√£o no data flow, control flow e ast matrices; por outro lado n√£o fa√ßo tokeniza√ß√£o com camel case e snake case e fa√ßo-o nas token e statement matrices)
        - Se quiser retirar sinais de pontua√ß√£o, usar `string.punctuation` (https://www.geeksforgeeks.org/string-punctuation-in-python/)
        - Na AST, retirar os n√≥s que s√£o pontua√ß√£o (ver edges do graph no c√≥digo)
        - VER LATER: len(code_tokens) > max_src_len ‚áí cortar sinais de pontua√ß√£o. Se ainda assim estiver maior, v√£o haver palavras a perderem-se‚Ä¶
    - VER LATER: Testar a tokeniza√ß√£o com o m√©todo do CodeSearchNet (para os code tokens)

    - TODO: Try different values for miu in attention heads and k in HSVA (optuna)
    - TODO: Refactor do c√≥digo atual ‚Üí Ver TODOs + dar fix quando se corre com v√°rias GPUs

    Depois dos TODOs de cima, devo:
    - Ver se faz sentido ter a AST head attention ou n√£o?

    Depois:

    - Ablation Studies
        - Se quiser desativar as matrizes, basta mudar o hyperparameter respetivo ou no caso das token/statement matrices, posso simplesmente substituir -1e9 por 0.
        - Talvez testar com outros pre-trained models (?)
    - Human evaluation
    - Evaluate baselines
    - Escrever a tese
    - FIM üòÄ

    Cenas fixes a fazer:

    - O script de build_dataset podia ter em cada JSON entry o code_snippet, summary, code_tokens, summary_tokens e tb a respetiva token, statement, data-flow, control-flow e AST adjacency matrix.

    ---

    - Porque √© que a validation loss n√£o se relaciona com os resultados das m√©tricas?
    - Avaliar o modelo com as m√©tricas apresentadas aqui: https://github.com/DeepSoftwareAnalytics/CodeSumEvaluation e com os datasets deles
    - Come√ßar a ver a copy attention (e relative positional encoding?) e como a colocar no modelo
    - Ver diferentes t√©cnicas de inicializa√ß√£o de pesos
    - Ver diferentes t√©cnicas de tokeniza√ß√£o e pre-processing (pre-processing est√° no Projeto de Tese)

    ---

    NOTAS:

    - (Pasta A_old tem as matrizes antes do data flow, control flow e AST terem os tokens snake_case e camelCase partidos em ‚Äún√≥s‚Äù)
    '''